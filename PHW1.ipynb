{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# find_Scaler_Model"
      ],
      "metadata": {
        "id": "Lbuvg8mKiwrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## find_Scaler_Model(train_X, train_Y)\n",
        "* Parameters:\n",
        "    - X: array-like of shape\n",
        "      - Training Vector\n",
        "    - y: array-like of shape\n",
        "      - Target relative to X for classfication\n",
        "* Returns:\n",
        "    - result_list: list\n",
        "      - Returns a list that contains result of various combination of scalers, models,and model hyper parameters.\n",
        "* Description:\n",
        "    - code for our funciton:"
      ],
      "metadata": {
        "id": "wbKk1vP5c5rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_Scaler_Model(X, y):\n",
        "\n",
        "  # Expected foramt = [Model name, Dictionary]\n",
        "  # Dictionary format = {\n",
        "  # score = best_score\n",
        "  # param = best_params\n",
        "  # model = model generated by best params\n",
        "  # scaler = scaler used   \n",
        "  # }\n",
        "  result_list = []\n",
        "\n",
        "  # *** Parameter descriptions ***\n",
        "  # C : Controls tradeoff detween smooth decision boundary and classfying training points correctly. So, C adjusts the margin. Smaller C allows more, larger C allows less\n",
        "  #\n",
        "  # Gamma: Defines how far the influence of a single training point reaches\n",
        "  # reach can think of reach as the range of data that affects the curvature of the decision boudary. If the gamma is small, it means the reach is far, and if the gamma is large, the reach is narrow.\n",
        "  # decision_function_shape : decision_function_shape : Whether to return a one-vs-rest (‘ovr’) decision function of shape as all other classifiers, or the original one-vs-one (‘ovo’) decision function.\n",
        "  #\n",
        "  # random_state : Controls the pseudo random number generation for shuffling the data for probability estimates. \n",
        "  # probability : gives per-class scores for each sample\n",
        "  #\n",
        "  # https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-3%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM-%EC%8B%A4%EC%8A%B5?category=1057680\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  params = {\n",
        "      'DTC_Gini': {\n",
        "          'criterion':['gini'],\n",
        "          'splitter':['best','random'],\n",
        "          'max_depth':[2,3,4,5,6,7,8],\n",
        "          'min_samples_split':[2,3,4,5]\n",
        "      },\n",
        "      'DTC_Entropy':{\n",
        "          'criterion':['entropy'],\n",
        "          'splitter':['best','random'],\n",
        "          'max_depth':[2,3,4,5,6,7,8],\n",
        "          'min_samples_split':[2,3,4,5]\n",
        "      },\n",
        "      'SVC': {\n",
        "          'kernel': [\"rbf\", \"poly\", \"sigmoid\", \"linear\"],\n",
        "          'gamma': [1e-3, 1e-2, 1e-1, 1, 1e+1],\n",
        "          'C': [1, 5, 10, 50, 100],\n",
        "          'decision_function_shape': [\"ovr\", \"ovo\"]\n",
        "      },\n",
        "      'LR': {\n",
        "          'solver': [\"lbfgs\", \"newton-cg\", \"liblinear\", \"sag\", \"saga\"],\n",
        "          \"penalty\": [\"l2\"],\n",
        "          'C': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
        "      }\n",
        "  }\n",
        "  # CV values\n",
        "  cv_k = set(range(2,11))\n",
        "  # Scalers using\n",
        "  scalers = [StandardScaler(), RobustScaler(), MinMaxScaler(), MaxAbsScaler()]\n",
        "  # Do scaling\n",
        "  scaled_dict = do_scaling(scalers, X)\n",
        "  # Decision Tree with gini\n",
        "  result_list += find_DecisionTree_gini(scaled_dict, y, cv_k, params['DTC_Gini'])\n",
        "  # Decision Tree with entropy\n",
        "  result_list += find_DecisionTree_entropy(scaled_dict, y, cv_k, params['DTC_Entropy'])\n",
        "  # SVC\n",
        "  result_list += find_SVC(scaled_dict, y, cv_k, params['SVC'])\n",
        "  # LogisticRegression\n",
        "  result_list += find_LogisticRegression(scaled_dict, y, cv_k, params['LR'])\n",
        "  # Sort list descending order to elements score\n",
        "  result_list.sort(key = lambda i : i[1]['score'], reverse=True)\n",
        "\n",
        "  return result_list"
      ],
      "metadata": {
        "id": "rC5tXIFecybG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Main function contains 5 sub functions: \n",
        "  - do_scaling, find_DecisionTree_gini, find_DecisionTree_entropy, find_SVC, find_LogisticRegression\n",
        "\n",
        "* Brief description about sub functions:\n",
        "  - Our main function first calls do_scaling function. do_scaling function returns a dictonary that contains scaler as a key and scaled dataframe as a value. Our team also added \"None\" key to the dictonary to test with non-scaled data. Then our main function calls find_DecisionTree_gini, find_DecisionTree_entropy, find_SVC, find_LogisticRegression. Each function returns a list that contains score, parameter, model of the result of each GridSearchCv. Using that results, our function can sort the result and returns the sorted result. Then we can use the best model to actually validate the model using test dataset.\n",
        "\n",
        "* code for our sub functions:\n",
        "  - do_scaling function code"
      ],
      "metadata": {
        "id": "yg66s1h-ejyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: generate scaled X using scalers\n",
        "# param scalers -> set: scalers to use (ex: [StandardScaler(), MinMaxScaler()])\n",
        "# param X -> pd.DataFrame: original dataset to be scaled\n",
        "# output: dict: scaled datasets\n",
        "def do_scaling(scalers, X):\n",
        "  scaled_X = {} # output value\n",
        "\n",
        "  for scaler in scalers: # do scaling for all scalers\n",
        "    scaled_X[scaler] = scaler.fit_transform(X.copy())\n",
        "  \n",
        "  scaled_X[\"None\"] = X.copy() # adding non-scaled data into dictonary\n",
        "  \n",
        "  return scaled_X"
      ],
      "metadata": {
        "id": "V5CLursZefKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* find_DecisionTree_gini function code:"
      ],
      "metadata": {
        "id": "Jqx318gzfG4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: hyperparameter tuning for decsion tree (with gini)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_DecisionTree_gini(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  dtc = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "  # output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      gd_sr = GridSearchCV(estimator=dtc, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = DecisionTreeClassifier(criterion='gini',\n",
        "                                          splitter=gd_sr.best_params_['splitter'],\n",
        "                                          max_depth=gd_sr.best_params_['max_depth'],\n",
        "                                          min_samples_split=gd_sr.best_params_['min_samples_split'],\n",
        "                                          random_state=1)\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "      \n",
        "      return_list.append(['DecisionTreeClassifier(criterion=\\'gini\\')', temp_list])\n",
        "\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "-FdTTREFe_31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* find_DecisionTree_entropy function code:"
      ],
      "metadata": {
        "id": "VUjNfUJkfUlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: hyperparameter tuning for decsion tree (with gini)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_DecisionTree_entropy(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  dtc = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "  # output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      gd_sr = GridSearchCV(estimator=dtc, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = DecisionTreeClassifier(criterion='entropy',\n",
        "                                          splitter=gd_sr.best_params_['splitter'],\n",
        "                                          max_depth=gd_sr.best_params_['max_depth'],\n",
        "                                          min_samples_split=gd_sr.best_params_['min_samples_split'],\n",
        "                                          random_state=1)\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "      \n",
        "      return_list.append(['DecisionTreeClassifier(criterion=\\'gini\\')', temp_list])\n",
        "\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "XlP5pWP0fQrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* find_SVC function code:"
      ],
      "metadata": {
        "id": "-FhTXp7vfdqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: hyperparameter tuning for SVM(SVC)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_SVC(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  svc = SVC(probability=True, random_state=100)\n",
        "\n",
        "  # Output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      # Do gridsearch\n",
        "      gd_sr = GridSearchCV(estimator=svc, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = SVC(kernel=gd_sr.best_params_['kernel'],\n",
        "                       gamma=gd_sr.best_params_['gamma'],\n",
        "                       C=gd_sr.best_params_['C'],\n",
        "                       decision_function_shape=gd_sr.best_params_['decision_function_shape'],\n",
        "                       probability=True,\n",
        "                       random_state=100)\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "\n",
        "      return_list.append(['SVC', temp_list])\n",
        "\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "l7TGyu1EfaTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* find_LogisticRegression funciton code:"
      ],
      "metadata": {
        "id": "EyFbti2RflxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: hyperparameter tuning for SVM(SVC)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_LogisticRegression(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  lr = LogisticRegression(random_state=1)\n",
        "\n",
        "  # Output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      # Do gridsearch\n",
        "      gd_sr = GridSearchCV(estimator=lr, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = LogisticRegression(solver=gd_sr.best_params_['solver'],\n",
        "                       random_state=1,\n",
        "                       penalty=gd_sr.best_params_['penalty'],\n",
        "                       C=gd_sr.best_params_['C'])\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "\n",
        "      return_list.append(['LogisticRegression', temp_list])\n",
        "\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "5xsYco6Xfh_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entire code we developed for this Programming HomeWork:\n",
        "  - Mount google drive"
      ],
      "metadata": {
        "id": "uPn1LZIcfwt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "DRIVE_PATH = '/content/drive'\n",
        "drive.mount(DRIVE_PATH)"
      ],
      "metadata": {
        "id": "qOio7giNfqZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries import"
      ],
      "metadata": {
        "id": "nRflk55uglCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "G4ykGBetgiT6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constant value definition"
      ],
      "metadata": {
        "id": "oYn8bW4Eg0ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = [\"?\"]\n",
        "\n",
        "columns = [\n",
        "'Simple code number',\n",
        "'Clump Thickness',\n",
        "'Uniformity of Cell Size',\n",
        "'Uniformity of Cell Shape',\n",
        "'Marginal Adhesion',\n",
        "'Single Epithelial Cell Size',\n",
        "'Bare Nuclei',\n",
        "'Bland Chromatin',\n",
        "'Normal Nucleoli',\n",
        "'Mitoses',\n",
        "'Class'\n",
        "]\n",
        "\n",
        "FILE_PATH = 'MyDrive/machine_learning/data'\n",
        "FILE_NAME = 'breast-cancer-wisconsin.data'\n",
        "\n",
        "target_column = 'Class'"
      ],
      "metadata": {
        "id": "-mDJou52gxLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data reads"
      ],
      "metadata": {
        "id": "UaK66iDIg8Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f\"{DRIVE_PATH}/{FILE_PATH}/{FILE_NAME}\", names=columns, na_values=missing_values)\n",
        "df"
      ],
      "metadata": {
        "id": "vopIvCXLg6AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df = df\n",
        "df = original_df.copy()\n",
        "df.info()\n",
        " \n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "vA7uFF7JhK20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "TXw_38schS_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Null\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "gVGCsN1AhQvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop an feature - results higher score\n",
        "df.drop([\"Simple code number\"], axis=1, inplace=True)\n",
        "\n",
        "# Change target(Class) value (2 -> 0 / 4 -> 1)\n",
        "df[target_column].replace([2, 4], [0, 1], inplace=True)\n",
        "\n",
        "# Split feature and target data\n",
        "X, y = df[df.columns.difference([target_column])], df[[target_column]].values.ravel()\n",
        "# print X\n",
        "X"
      ],
      "metadata": {
        "id": "t8hbYljvhiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print y\n",
        "df[[target_column]]"
      ],
      "metadata": {
        "id": "ZnHzTmVUhsUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Find Best model and options\n",
        "# Run findBestOptions()\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.7, shuffle=True)"
      ],
      "metadata": {
        "id": "CgypR8kkhy-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling functions"
      ],
      "metadata": {
        "id": "sBd2YVzIh007"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: generate scaled X using scalers\n",
        "# param scalers -> set: scalers to use (ex: [StandardScaler(), MinMaxScaler()])\n",
        "# param X -> pd.DataFrame: original dataset to be scaled\n",
        "# output: dict: scaled datasets\n",
        "def do_scaling(scalers, X):\n",
        "  scaled_X = {} # output value\n",
        "\n",
        "  for scaler in scalers: # do scaling for all scalers\n",
        "    scaled_X[scaler] = scaler.fit_transform(X.copy())\n",
        "  \n",
        "  scaled_X[\"None\"] = X.copy()\n",
        "  \n",
        "  return scaled_X\n",
        "# TODO: hyperparameter tuning for decsion tree (with gini)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_DecisionTree_gini(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  dtc = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "  # output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      gd_sr = GridSearchCV(estimator=dtc, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = DecisionTreeClassifier(criterion='gini',\n",
        "                                          splitter=gd_sr.best_params_['splitter'],\n",
        "                                          max_depth=gd_sr.best_params_['max_depth'],\n",
        "                                          min_samples_split=gd_sr.best_params_['min_samples_split'],\n",
        "                                          random_state=1)\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "      \n",
        "      return_list.append(['DecisionTreeClassifier(criterion=\\'gini\\')', temp_list])\n",
        "\n",
        "  return return_list\n",
        "# TODO: hyperparameter tuning for decsion tree (with gini)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_DecisionTree_entropy(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  dtc = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "  # output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      gd_sr = GridSearchCV(estimator=dtc, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = DecisionTreeClassifier(criterion='entropy',\n",
        "                                          splitter=gd_sr.best_params_['splitter'],\n",
        "                                          max_depth=gd_sr.best_params_['max_depth'],\n",
        "                                          min_samples_split=gd_sr.best_params_['min_samples_split'],\n",
        "                                          random_state=1)\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "      \n",
        "      return_list.append(['DecisionTreeClassifier(criterion=\\'gini\\')', temp_list])\n",
        "\n",
        "  return return_list\n",
        "# TODO: hyperparameter tuning for SVM(SVC)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_SVC(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  svc = SVC(probability=True, random_state=100)\n",
        "\n",
        "  # Output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      # Do gridsearch\n",
        "      gd_sr = GridSearchCV(estimator=svc, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = SVC(kernel=gd_sr.best_params_['kernel'],\n",
        "                       gamma=gd_sr.best_params_['gamma'],\n",
        "                       C=gd_sr.best_params_['C'],\n",
        "                       decision_function_shape=gd_sr.best_params_['decision_function_shape'],\n",
        "                       probability=True,\n",
        "                       random_state=100)\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "\n",
        "      return_list.append(['SVC', temp_list])\n",
        "\n",
        "  return return_list\n",
        "# TODO: hyperparameter tuning for SVM(SVC)\n",
        "# param scaler_dict -> dict: dictionary contains the scaler and scaled data used it\n",
        "# param y -> pd.DataFrame: target dataset\n",
        "# param cv -> set: CV integers\n",
        "# param params -> dict: parameters for hyperparameter tuning\n",
        "def find_LogisticRegression(scaler_dict, y, cv, params):\n",
        "  # Model\n",
        "  lr = LogisticRegression(random_state=1)\n",
        "\n",
        "  # Output\n",
        "  return_list = []\n",
        "\n",
        "  # Do the same modeling for input scalers\n",
        "  for scaler, scaled_X in scaler_dict.items():\n",
        "    for i in cv: # various cv: from 2 to 10\n",
        "      # Do gridsearch\n",
        "      gd_sr = GridSearchCV(estimator=lr, param_grid=params, cv=i, n_jobs=-1)\n",
        "      gd_sr.fit(scaled_X, y)\n",
        "\n",
        "      # Push output value\n",
        "      temp_list = {\n",
        "          'score': gd_sr.best_score_,\n",
        "          'param': gd_sr.best_params_,\n",
        "          'scaler': scaler\n",
        "      }\n",
        "\n",
        "      # Add model\n",
        "      temp_model = LogisticRegression(solver=gd_sr.best_params_['solver'],\n",
        "                       random_state=1,\n",
        "                       penalty=gd_sr.best_params_['penalty'],\n",
        "                       C=gd_sr.best_params_['C'])\n",
        "      temp_model.fit(scaled_X, y)\n",
        "      temp_list['model'] = temp_model\n",
        "\n",
        "      return_list.append(['LogisticRegression', temp_list])\n",
        "\n",
        "  return return_list\n",
        "def find_Scaler_Model(X, y):\n",
        "\n",
        "  # Expected foramt = [Model name, Dictionary]\n",
        "  # Dictionary format = {\n",
        "  # score = best_score\n",
        "  # param = best_params\n",
        "  # model = model generated by best params\n",
        "  # scaler = scaler used   \n",
        "  # }\n",
        "  result_list = []\n",
        "\n",
        "  # *** Parameter descriptions ***\n",
        "  # C : Controls tradeoff detween smooth decision boundary and classfying training points correctly. So, C adjusts the margin. Smaller C allows more, larger C allows less\n",
        "  #\n",
        "  # Gamma: Defines how far the influence of a single training point reaches\n",
        "  # reach can think of reach as the range of data that affects the curvature of the decision boudary. If the gamma is small, it means the reach is far, and if the gamma is large, the reach is narrow.\n",
        "  # decision_function_shape : decision_function_shape : Whether to return a one-vs-rest (‘ovr’) decision function of shape as all other classifiers, or the original one-vs-one (‘ovo’) decision function.\n",
        "  #\n",
        "  # random_state : Controls the pseudo random number generation for shuffling the data for probability estimates. \n",
        "  # probability : gives per-class scores for each sample\n",
        "  #\n",
        "  # https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-3%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0-SVM-%EC%8B%A4%EC%8A%B5?category=1057680\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  params = {\n",
        "      'DTC_Gini': {\n",
        "          'criterion':['gini'],\n",
        "          'splitter':['best','random'],\n",
        "          'max_depth':[2,3,4,5,6,7,8],\n",
        "          'min_samples_split':[2,3,4,5]\n",
        "      },\n",
        "      'DTC_Entropy':{\n",
        "          'criterion':['entropy'],\n",
        "          'splitter':['best','random'],\n",
        "          'max_depth':[2,3,4,5,6,7,8],\n",
        "          'min_samples_split':[2,3,4,5]\n",
        "      },\n",
        "      'SVC': {\n",
        "          'kernel': [\"rbf\", \"poly\", \"sigmoid\", \"linear\"],\n",
        "          'gamma': [1e-3, 1e-2, 1e-1, 1, 1e+1],\n",
        "          'C': [1, 5, 10, 50, 100],\n",
        "          'decision_function_shape': [\"ovr\", \"ovo\"]\n",
        "      },\n",
        "      'LR': {\n",
        "          'solver': [\"lbfgs\", \"newton-cg\", \"liblinear\", \"sag\", \"saga\"],\n",
        "          \"penalty\": [\"l2\"],\n",
        "          'C': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
        "      }\n",
        "  }\n",
        "\n",
        "  # CV values\n",
        "  cv_k = set(range(2,11))\n",
        "\n",
        "  # Scalers using\n",
        "  scalers = [StandardScaler(), RobustScaler(), MinMaxScaler(), MaxAbsScaler()]\n",
        "\n",
        "  # Do scaling\n",
        "  scaled_dict = do_scaling(scalers, X)\n",
        "\n",
        "  # Decision Tree with gini\n",
        "  result_list += find_DecisionTree_gini(scaled_dict, y, cv_k, params['DTC_Gini'])\n",
        "\n",
        "  # Decision Tree with entropy\n",
        "  result_list += find_DecisionTree_entropy(scaled_dict, y, cv_k, params['DTC_Entropy'])\n",
        "\n",
        "  # SVC\n",
        "  result_list += find_SVC(scaled_dict, y, cv_k, params['SVC'])\n",
        "\n",
        "  # LogisticRegression\n",
        "  result_list += find_LogisticRegression(scaled_dict, y, cv_k, params['LR'])\n",
        "\n",
        "  # Sort list descending order to elements score\n",
        "  result_list.sort(key = lambda i : i[1]['score'], reverse=True)\n",
        "\n",
        "  return result_list"
      ],
      "metadata": {
        "id": "4JztkvxOhfom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "GJZ4C-gRiEXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = find_Scaler_Model(train_X, train_y)\n",
        "# printing top 5 result of the function\n",
        "for i in range(5):\n",
        "  print(result[i])"
      ],
      "metadata": {
        "id": "7jJh3QUPiDSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ['SVC', {'score': 0.9950980392156863, 'param': {'C': 1, 'decision_function_shape': 'ovr', 'gamma': 0.001, 'kernel': 'linear'}, 'scaler': StandardScaler(), 'model': SVC(C=1, gamma=0.001, kernel='linear', probability=True, random_state=100)}]\n",
        "* ['SVC', {'score': 0.9950980392156863, 'param': {'C': 50, 'decision_function_shape': 'ovr', 'gamma': 0.01, 'kernel': 'rbf'}, 'scaler': RobustScaler(), 'model': SVC(C=50, gamma=0.01, probability=True, random_state=100)}]\n",
        "* ['SVC', {'score': 0.9950980392156863, 'param': {'C': 5, 'decision_function_shape': 'ovr', 'gamma': 0.001, 'kernel': 'linear'}, 'scaler': MinMaxScaler(), 'model': SVC(C=5, gamma=0.001, kernel='linear', probability=True, random_state=100)}]\n",
        "* ['SVC', {'score': 0.9950980392156863, 'param': {'C': 50, 'decision_function_shape': 'ovr', 'gamma': 0.1, 'kernel': 'sigmoid'}, 'scaler': MinMaxScaler(), 'model': SVC(C=50, gamma=0.1, kernel='sigmoid', probability=True, random_state=100)}]\n",
        "* ['SVC', {'score': 0.9950980392156863, 'param': {'C': 5, 'decision_function_shape': 'ovr', 'gamma': 0.001, 'kernel': 'linear'}, 'scaler': MaxAbsScaler(), 'model': SVC(C=5, gamma=0.001, kernel='linear', probability=True, random_state=100)}]\n"
      ],
      "metadata": {
        "id": "a96M1Yn1iVoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if result[0][1]['scaler'] == \"None\":\n",
        "  scaled_X = X.copy()\n",
        "else :\n",
        "  scaled_X = result[0][1]['scaler'].fit_transform(X.copy())\n",
        "train_X, test_X, train_y, test_y = train_test_split(scaled_X, y, test_size=0.7, shuffle=True)\n",
        "model = result[0][1]['model'].fit(train_X, train_y)\n",
        "print(\"Model score: \", end=\"\")\n",
        "print(model.score(test_X, test_y))"
      ],
      "metadata": {
        "id": "XnYRUmu9iMPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}